# Configuration for the transcription tool
models:
  - parakeet
  - whisperx
  - stablew

output_formats:
  - json

output_path: "E:/_FLER-INTERVIEWS"  # Base directory for all outputs

model_configs:
  parakeet:
    active: true
    model_size: "nvidia/parakeet-tdt-0.6b-v3"
    device: "cpu" # cpu/cuda
    # Memory optimization settings for long audio files
    enable_local_attention: true
    local_attention_window: [128, 128]  # [left_context, right_context] in frames (stanni 128 / 128)
    enable_chunking: true
    use_multigpu: false  # Enable DataParallel for multiple GPUs
  
  whisperx:
    active: true
    model_size: "large-v3"  # tiny, base, small, medium, large-v2, large-v3
    device: "cpu"  # cpu/cuda
    compute_type: "int8"  # float16 for GPU, int8 for CPU
    batch_size: 16  # Batch size for transcription
    language: "de"  # Language code (de=German, en=English, etc.)
    enable_alignment: true  # Enable word-level timestamps via forced alignment
    enable_diarization: false  # Enable speaker diarization (requires hf_token)
    hf_token: null  # Hugging Face token for diarization (pyannote.audio)
    min_speakers: null  # Minimum number of speakers (for diarization)
    max_speakers: null  # Maximum number of speakers (for diarization)
  
  stablew:
    active: true
    model_size: "large-v3"  # tiny, base, small, medium, large-v2, large-v3
    device: "cuda"  # cpu/cuda
    # Note: stable-ts doesn't use compute_type, it auto-selects based on device
    language: "de"  # Language code (de=German, en=English, etc.)
    # Voice Activity Detection
    vad: true  # Enable VAD for better speech detection
    vad_threshold: 0.35  # VAD threshold (0.0-1.0, lower = more sensitive)
    vad_onnx: false  # Use ONNX for faster VAD (requires onnxruntime)
    # Timestamp stabilization (eliminates overlaps)
    suppress_silence: true  # Suppress timestamps in silent parts
    suppress_word_ts: true  # Suppress unreliable word timestamps
    min_word_dur: 0.1  # Minimum word duration in seconds
    max_word_dur: 3.0  # Maximum word duration in seconds
    nonspeech_error: 0.1  # Error tolerance for non-speech detection
    # Post-processing
    refine: false  # Refine timestamps after transcription (VERY slow, 4+ hours for long audio)
    only_voice_freq: false  # Filter out non-voice frequencies (slower but more accurate)
    # Segment merging (combine segments into complete sentences)
    regroup: true  # Regroup segments by punctuation (., !, ?) - RECOMMENDED
    merge_by_sentence: false  # Alternative merging method (only if regroup is false)
    max_sentence_len: null  # Maximum sentence length in characters (null = no limit)

# Ultra quality configurations (GPU, maximum quality)
ultra_configs:
  parakeet:
    device: "cuda"  # cpu/cuda
    enable_local_attention: false  # Disable for maximum quality
    enable_chunking: false         # Disable for maximum quality
    batch_size: 4                  # Larger batch size for GPU (was 4)
    use_multigpu: true             # Enable multi-GPU in ultra mode
  
  whisperx:
    device: "cuda"  # cpu/cuda
    compute_type: "float16"  # float16 for maximum quality on GPU
    batch_size: 16  # Larger batch for GPU
    enable_alignment: true  # Keep word-level timestamps
    enable_diarization: false  # Optional: enable if you need speaker identification
  
  stablew:
    device: "cuda"  # cpu/cuda
    # Note: stable-ts doesn't use compute_type
    vad_onnx: true  # Use ONNX for faster VAD on GPU
    refine: true  # Always refine in ultra mode for best accuracy

youtube_download:
  video_quality: "bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best"
  # Additional video versions to download/convert
  additional_versions:
    - quality: "480p"  # Download or convert to 480p
      format: "best[height<=480][ext=mp4]/best[ext=mp4]/best"  # yt-dlp format selector
      convert_from_best: false  # If true, convert from best quality instead of downloading separately
  audio_extraction:
    acodec: "pcm_s16le"
    ar: 16000 # sampling rate
    ac: 1 # mono
